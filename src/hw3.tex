\lhead{\textbf{Basic Algorithms, Fall 2024 \\ CSCI-UA.0310-001}}
\chead{\Large{\textbf{Homework 3}}}
\def\lc{\left\lceil}   
\def\rc{\right\rceil}
\rhead{\textbf{Instructor: Rotem Oshman \\ Name: Ishan Pranav}}
\runningheadrule
\firstpageheadrule
\cfoot{}
\subsection*{References}
Collaborated with Crystal Huang.
\subsection*{Question 1: {\sc InsertionSort} \textit{vs.} {\sc QuickSort}}

Recall that in the worst case the running time of (non-randomized) {\sc QuickSort}, that uses the last element as the pivot, and {\sc InsertionSort} are $\Theta(n^2)$. Refer to the lecture notes for the {\sc QuickSort} implementation.

\begin{enumerate}
    \item Give an example of an array of length $n$ where both take time $\Omega(n^2)$. Justify your answer.  
\begin{solution}
Let $A[1,\dots n]$ be an $n$-element array where $A[i]\geq A[i+1]$ for all $1\leq i<n$.
\end{solution}
    \item Give an example of an array of length $n$ where (non-randomized) {\sc QuickSort} runs in $\Omega(n^2)$ but {\sc Insertion-Sort} runs in time $O(n)$. Justify your answer. 
\begin{solution}
Let $A[1,\dots n]$ be an $n$-element array where $A[i]\leq A[i+1]$ for all $1\leq i<n$.
\end{solution}
\end{enumerate}
\newpage
\subsection*{Question 2: Fast exponentiation}
In this problem, you will design an algorithm to compute $2024^n$ given $n \in \mathbb{N}$ as input. In each case, prove the correctness of your algorithm, and an upper bound on the number of multiplications used.
\begin{enumerate}
    \item Using $n-1$ many multiplications.
\begin{solution}
{\sc NaiveExponentiation}($n$) where $n\in\mathbb{N}$:
\begin{itemize}
\item if $n=1$, then return $2024$;
\item otherwise, return the product $2024~\times~${\sc NaiveExponentiation}($n-1$).
\end{itemize}
\textbf{Lemma I. }

\textit{Claim. }{\sc NaiveExponentiation}($n$)$~=2024^n$ for all $n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. We have {\sc NaiveExponentiation}($1$)$~=2024=2024^1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=k$ where $k>1$. Assume that {\sc NaiveExponentiation}($k$)$~=2024^k$.

\textit{Inductive step. }Consider $n=k+1$. Then

{\sc NaiveExponentiation}($k+1$)$~=2024~\times~${\sc NaiveExponentiation}($(k+1)-1$). That is,

{\sc NaiveExponentiation}($k+1$)$~=2024~\times~${\sc NaiveExponentiation}($k$).

By the inductive hypothesis, we have {\sc NaiveExponentiation}($k$)$~=2024^k$. Thus

{\sc NaiveExponentiation}($k+1$)$~=2024\times2024^k$. Ergo,

{\sc NaiveExponentiation}($k+1$)$~=2024^{k+1}$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc NaiveExponentiation} algorithm produces the correct result for all $n\in\mathbb{N}$.\\

\textbf{Lemma II. }

\textit{Claim. }{\sc NaiveExponentiation}($n$) uses at most $n-1$ multiplications for all $n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. Since $n=1$, the number of multiplications is $0=1-1\leq n-1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=k$ where $k>1$. Assume that {\sc NaiveExponentiation}($k$) uses at most $k-1$ multiplications.

\textit{Inductive step. }Consider $n=k+1$. Then the number of multiplications is $1$, plus the number of multiplications used by {\sc NaiveExponentiation}($(k+1)-1$). That is, $1$ plus the number of multiplications used by {\sc NaiveExponentiation}($k$). By the inductive hypothesis, the number of multiplications used by {\sc NaiveExponentiation}($k$) is at most $k-1$. Therefore, the number of multiplications used by {\sc NaiveExponentiation}($k+1$) is at most $1+(k-1)=(k+1)-1$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc NaiveExponentiation} algorithm uses at most $n-1$ multiplications for all $n\in\mathbb{N}$.\\

\textbf{Proof. }

By Lemma I, {\sc NaiveExponentiation}($n$) computes $2024^n$ for all $n\in\mathbb{N}$.

By Lemma II, {\sc NaiveExponentiation}($n$) uses at most $n-1$ multiplications for all $n\in\mathbb{N}$.

Ergo, for all $n\in\mathbb{N}$, we have demonstrated that {\sc NaiveExponentiation}($n$) computes $2024^n$ using at most $n-1$ multiplications.$~\square$
\end{solution}
\newpage
    \item Using $O(\log_2 n)$ many multiplications, assuming $n$ is a power of $2$, i.e., $n=2^k$.
\begin{solution}
{\sc SquareExponentiation}($n$) where $n=2^x$ for $x\geq 0$:
\begin{itemize}
\item if $n=2^0$, then return $2024$;
\item otherwise, compute $y~=~${\sc SquareExponentiation}($n/2$), and return $y\times y$.
\end{itemize}
\textbf{Lemma I. }

\textit{Claim. }{\sc SquareExponentiation}($n$)$~=2024^n$ for $n=2^x$ where $x\geq 0$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=2^0$. We have {\sc SquareExponentiation}($2^0$)$~=2024=2024^1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=2^k$ where $k>0$. Assume that {\sc SquareExponentiation}($2^k$)$~=2024^{2^k}$.

\textit{Inductive step. }Consider $n=2^{k+1}$. Then

$y=~${\sc SquareExponentiation}($2^{k+1}/2$)$~=~${\sc SquareExponentiation}($2^k$). By the inductive hypothesis, we have
{\sc SquareExponentiation}($2^k$)$~=~2024^{2^k}$. Thus $y=2024^{2^k}$. Now

{\sc SquareExponentiation}($2^{k+1}$)$~=~y\times y$, so {\sc SquareExponentiation}($2^{k+1}$)$~=2024^{2^k}\times 2024^{2^k}=2024^{2^k+2^k}=2024^{2\cdot 2^k}=2024^{2^{k+1}}$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc SquareExponentiation} algorithm produces the correct result for all $n=2^x$ where $x\geq 0$.\\

\textbf{Lemma II. }

\textit{Claim. }{\sc SquareExponentiation}($n$) uses $O(\log_2n)$ multiplications for $n=2^x$ where $x\geq 0$. Let $M(n)$ denote the number of multiplications used by {\sc Square Exponentiation}($n$). We claim that $M(n)=O(\log_2n)$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=2^0$. Then there are no multiplications, so $M(2^0)=0$. Since $\log_2n\geq 0$ for $n\geq 1$, we can choose any positive constant $C$ and any $n_0\geq 1$ such that $0\leq C\log_2n$ for $n>n_0$. Therefore $M(n)=O(\log_2n)$ holds in the base case.

\textit{Hypothesis. }Consider $n=2^k$ where $k>0$. Assume that $M(2^k)=O(\log_2n)$.

\textit{Inductive step. }Consider $n=2^{k+1}$. Then the number of multiplications is $1$ (to compute $y\times y$), plus the number of multiplications used by the recursive call. That is, 
\begin{align*}
M(2^{k+1})&=1+M\left(\frac{2^{k+1}}{2}\right)\\
&=1+M(2^k).
\end{align*}

By the inductive hypothesis, we have $M(2^k)=O(\log_2n)$. Asymptotically,
\begin{align*}
M(2^{k+1})&=1+O(\log_2n)\\
&=O(1)+O(\log_2n)\\
&=O(\log_2n).
\end{align*}

This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc SquareExponentiation} algorithm uses $O(\log_2n)$ many multiplications for all $n=2^x$ where $x\geq 0$.\\

\textbf{Proof. }

By Lemma I, {\sc SquareExponentiation}($n$) computes $2024^n$ for all $n=2^x$ where $x\geq 0$.

By Lemma II, {\sc SquareExponentiation}($n$) uses at $O(\log_2n)$ many multiplications for all $n=2^x$ where $x\geq 0$.

Ergo, for all $n=2^x$ where $x\geq 0$, we have demonstrated that {\sc SquareExponentiation}($n$) computes $2024^n$ using $O(\log_2n)$ many multiplications.$~\square$
\end{solution}
\newpage
\item Using $O(\log_2 n)$ many multiplications for \emph{any} $n$ (not necessarily a power of $2$).
\begin{solution}\\

\textbf{Algorithm I. }{\sc FastExponentiation}($b,n$) where $n\in\mathbb{N}$:
\begin{itemize}
\item if $n=1$, then return $b$;
\item otherwise, compute $b'=b\times b$, and:
\begin{itemize}
    \item if $n$ is even, then return {\sc FastExponentiation}($b',n/2$);
    \item otherwise, return the product $b~\times~${\sc FastExponentiation}($b',(n-1)/2$).
\end{itemize}
\end{itemize}

\textbf{Algorithm II. }{\sc FastPow2024($n$)} where $n\in\mathbb{N}$: return the result {\sc FastExponentiation}($2024,n$).\\

\textbf{Lemma I.}

\textit{Claim. }{\sc FastExponentiation}($b,n$)$~=b^n$ for all $b,n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. We have {\sc FastExponentiation}($b,1$)$~=b=b^1$ for all $b\in\mathbb{N}$. The claim holds in the base case.

\textit{Hypothesis. }Consider $1<n<k$. Assume that {\sc FastExponentiation}($b,k$)$~=b^k$ for all $b\in\mathbb{N}$.

\textit{Inductive step. }Consider $n=k+1$. Since $k+1>1$, we compute $b'=b\times b$. Of course, since $b\in\mathbb{N}$, we have $b'\in\mathbb{N}$. Since $k\in\mathbb{N}$, either $k+1$ is even or $k+1$ is odd:
\begin{itemize}
\item Suppose $k+1$ is even. Now,

{\sc FastExponentiation}($b,k+1$)$~=~${\sc FastExponentiation}($b',\frac{k+1}{2}$).

Since $k+1\in\mathbb{N}$ and $k+1$ is even, we have $\frac{k+1}{2}\in\mathbb{N}$. By the strong induction hypothesis, {\sc FastExponentiation}($b',\frac{k+1}{2}$)$~={b'}^{\frac{k+1}{2}}=(b\times b)^{\frac{k+1}{2}}=(b^2)^{\frac{k+1}{2}}=b^{k+1}$. Ergo

{\sc FastExponentiation}($b,k+1$)$~=b^{k+1}$.
\item Suppose instead $k+1$ is odd. Now,

{\sc FastExponentiation}($b,k+1$)$~=b~\times~${\sc FastExponentiation}($b',\frac{(k+1)-1}{2}$).

Since $k+1\in\mathbb{N}$ and $k+1$ is odd, we have $\frac{(k+1)-1}{2}\in\mathbb{N}$. By the strong induction hypothesis, {\sc FastExponentiation}($b',\frac{(k+1)-1}{2}$)$~={b'}^{\frac{(k+1)-1}{2}}={b'}^{\frac{k}{2}}=(b\times b)^{\frac{k}{2}}=(b^2)^{\frac{k}{2}}=b^k$. Ergo

{\sc FastExponentiation}($b,k+1$)$~=b\times b^k=b^{k+1}$.
\end{itemize}

In all cases, {\sc FastExponentiation}($b,k+1$)$~=b^{k+1}$ for all $b\in\mathbb{N}$. This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc FastExponentiation} algorithm produces the correct result for all $b,n\in\mathbb{N}$.\\

\textbf{Lemma II.}

\textit{Claim. }{\sc FastExponentiation}($b,n$)$~=b^n$ uses $O(\log_2n)$ multiplications for all $b,n\in\mathbb{N}$. Let $M(b,n)$ denote the number of multiplications used by {\sc FastExponentiation}($b,n$). We claim that $M(b,n)=O(\log_2n)$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. Then, regardless of the base $b$, there are no multiplications, so $M(b,1)=0$ for all $b\in\mathbb{N}$. Since $\log_2n\geq 0$ for $n\geq 1$, we can choose any positive constant $C$ and any $n_0\geq 1$ such that $0\leq C\log_2n$ for $n\geq n_0$. Therefore $M(b,n)=O(\log_2n)$ holds for all $b\in\mathbb{N}$ in the base case.

\textit{Hypothesis. }Consider $1<n<k$. Assume that $M(b,k)=O(\log_2 n)$ for all $b\in\mathbb{N}$.

\textit{Inductive step. }Consider $n=k+1$. Since $k+1>1$, we know $1$ multiplication is required to compute $b'=b\times b$. Since $k\in\mathbb{N}$, either $k+1$ is even or $k+1$ is odd:
\begin{itemize}
\item Suppose $k+1$ is even. The number of additional multiplications is $M\left(b',\frac{k+1}{2}\right)$. By the strong induction hypothesis, we have $M\left(b',\frac{k+1}{2}\right)=O(\log_2n)$.
\item Suppose instead $k+1$ is odd. The number of additional multiplications is $M\left(b',\frac{(k+1)-1}{2}\right)$. By the strong induction hypothesis, we have $M\left(b',\frac{(k+1)-1}{2}\right)=M\left(b',\frac{k}{2}\right)=O(\log_2n)$.
\end{itemize}
In all cases, the number of additional multiplications is $O(\log_2n)$. Considering also the single multiplication used to compute $b'=b\times b$, asymptotically we have
\begin{align*}
M(b,k+1)&=1+O(\log_2n)\\
&=O(1)+O(\log_2n)\\
&=O(\log_2n).
\end{align*}
This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc FastExponentiation} algorithm uses $O(\log_2n)$ many multiplications for all $b,n\in\mathbb{N}$.\\

\textbf{Proof.}

By construction, {\sc FastPow2024}($n$) algorithm returns {\sc FastExponentiation}($2024,n$) and uses exactly as many multiplications as the {\sc FastExponentiation}($2024,n$) algorithm.

By Lemma I, {\sc FastExponentiation}($2024,n$) computes $2024^n$ for all $n\in\mathbb{N}$. Therefore, the {\sc FastPow2024} algorithm computes $2024^n$ for all $n\in\mathbb{N}$.

By Lemma II, {\sc FastExponentiation}($2024,n$) uses $O(\log_2n)$ many multiplications for all $n\in\mathbb{N}$. Therefore, the {\sc FastPow2024} algorithm uses $O(\log_2n)$ many multiplications for all $n\in\mathbb{N}$.

Ergo, for all $n\in\mathbb{N}$, we have demonstrated that {\sc FastPow2024}($n$) computes $2024^n$ using $O(\log_2n)$ many multiplications.$~\square$
\end{solution}
\newpage
\end{enumerate}
\subsection*{Question 3: Close to sorted}
Suppose you are given a list $A$ of $n$ distinct numbers. You are guaranteed that this list is `close to sorted' in the following sense: if $A_{\text{sorted}}$ denotes the list fully sorted in increasing order, then $A_{\text{sorted}}$ differs from $A$ in at most $\log_2 n$ many positions. Intuitively, this says that at most $\log_2 n$ elements are out of place in the original list. For instance, in the list 
\[
(2,5,9,11,20,14,15,12,25,30)
\]
only two elements are out of place (20 and 12), since after sorting, we get 
\[
(2,5,9,11,12,14,15,20,25,30) \;.
\]
The following exercises will ultimately lead to an algorithm which sorts $A$ in time $O(n)$~\footnote{Note that sorting algorithms take $O(n\log n)$ time without any assumptions on the inputs. Here, we are able to get the faster $O(n)$ run-time by \emph{assuming} that the input array $A$ is already close to being sorted.}. Answer each exercise.

\begin{enumerate}
    \item Prove that the leftmost (first) out of place element must be too big (not too small) for its place.
\begin{solution}
\textit{Claim. }Let $k$ represent the position of the leftmost out-of-place element in $A$. Then $A[k]$ is too big (not too small) for its position.

\textit{Proof. }Assume, for the sake of contradiction, that $k$ is too small for its position. Since $A[k]$ is too small for its position, we know $k>1$. Otherwise, if $k=1$, it could not be too small for its position. Of course, since $A[k]$ is too small for its position, $A[k]<A[k-1]$.

This implies that $A[k-1]>A[k]$, meaning that $A[k-1]$ is too big for its place. $A[k-1]$ is out of place and $k-1<k$. Therefore, $A[k-1]$ is out of place and \textit{further left} than $A[k]$. This contradicts the hypothesis that $A[k]$ is the leftmost out-of-place element in $A$.

We conclude that the leftmost out-of-place element in $A$ must \textit{not} be too small for its place. Hence, the leftmost out-of-place element in $A$ must be too big for its place.$~\square$
\end{solution}
    \item Suppose we construct a stack $S$ as follows: We go through $A$ from left to right, pushing elements one by one into $S$ as long as the next element from $A$ is larger than the top element $s$ of the stack so far. If at any step $i$ the element $A[i]$ which we are currently considering in $A$ is smaller than $s$, we instead pop $s$ from $S$ and continue.
    This idea is described in the pseudo code below:
    \begin{minipage}{\linewidth}
    \begin{algorithm}[H]
    \label{incsubseq}
    \caption{Construct increasing subsequence $S$}
    \begin{algorithmic}
    \REQUIRE $A$ and an empty stack $S$.
    \ENSURE $S$ representing an increasing subsequence of $A$
    \STATE $n \leftarrow \text{len}(A)$
    \FOR {$i = 1 \text{ to } n$}
        \IF {$S$ is empty \OR $S.\text{topElement}() \leq A[i]$}
        \STATE $S.\text{push}(A[i])$
        \ELSE 
        \STATE $S.\text{pop}()$
        \ENDIF
    \ENDFOR
    \STATE \RETURN $S$
    \end{algorithmic}
    \end{algorithm}
    \end{minipage}
    \smallskip
    
    If $A$ is initially $(2,5,9,11,20,14,15,12,25,30)$, what will $S$ be after Algorithm 1 has run?
\begin{solution}
Let $A=(2,5,9,11,20,14,15,12,25,30)$. Let $S$ be an empty stack. 

Let $S_i$ denote $S$ before iteration $1\leq i\leq\text{len}(A)+1$. For example, $S_1$ represents the stack before iteration $1$ and $S_2$ represents the stack after iteration $1$ but before iteration $2$. $S_{n+1}$ represents $S$ after the final iteration. Stepping through the algorithm:
\begin{align*}
S_1&=()&\textit{before the first iteration.}\\
S_2&=(2)&\textit{since $S_1$ is empty.}\\
S_3&=(2,5)&\textit{since $2\leq 5$.}\\
~&\vdots&\vdots\\
S_6&=(2,5,9,11,20)&\textit{since $5\leq 9\leq 11\leq 20$.}\\
S_7&=(2,5,9,11)&\textit{since $S$ is not empty, $20\nleq 14$.}\\
S_8&=(2,5,9,11,15)&\textit{since $11\leq 15$.}\\
S_9&=(2,5,9,11)&\textit{since $S$ is not empty, $15\nleq 12$.}\\
~&\vdots&\vdots\\
S_{11}&=(2,5,9,11,25,30)&\textit{since $11\leq 25\leq 30$.}
\end{align*}
After Algorithm 1 has run, $S=(2,5,9,11,25,30)$.
\end{solution}
\item Prove that $S$ is an increasing subsequence of $A$ (the elements of the output stack are increasing from bottom to top).
\begin{solution}\textbf{Invariant I.}

\textit{Claim. }Let $A$ be an array and $S$ be an empty stack. Consider Algorithm 1.

Let $S_i$ denote $S$ before iteration $1\leq i\leq\text{len}(A)+1$. For example, $S_1$ represents the stack before iteration $1$ and $S_2$ represents the stack after iteration $1$ but before iteration $2$. 

We want to demonstrate that, for every iteration of the loop in Algorithm 1, the stack $S$ is a monotonically increasing subsequence of $A$.

That is, for all $i\leq\text{len}(A)+1$, for all $s\in S_i$, we have $s\in A$, and for all $1\leq j\leq\text{len}(S_i)$, we have $S_i[j]\leq S_i[j+1]$.

We can prove this invariant by induction on $i$.

\textit{Basis. }Consider $i=1$ (before iteration $1$). Here $S_1$ is an empty stack. Vacuously, for all $s\in S_1$, we have $s\in A$. Similarly, for all $1\leq j<\text{len}(S_1)$, we have $S_1[j]\leq S_1[j+1]$. The invariant holds in the base case.

\textit{Hypothesis. }Consider $1<i\leq k\leq\text{len}(A)$ (before iteration $k$). Assume that for all $s\in S_k$, we have $s\in A$, and for all $1\leq j\leq\text{len}(S_k)$, we have $S_k[j]\leq S_k[j+1]$.

\textit{Inductive step. }Consider $i=k+1$ (after iteration $k$, but before iteration $k+1$):
\begin{itemize}
\item Suppose $k+1=\text{len}(A)+1$. Then the loop terminated after iteration $k$. We have $S_{k+1}=S_k$, so, by the induction hypothesis, the loop invariant holds.
\item Suppose instead $k+1\leq\text{len}(A)$. Then the loop body executed after iteration $k$:
\begin{itemize}
\item Suppose $S_k$ was empty or $S_k[\text{len}(S_k)]\leq A[k]$. Note $S_k[\text{len}(S_k)]$ denotes the top element of the stack. Then the top element of the stack has been updated such that $S_{k+1}[\text{len}(S_{k+1})]=A[k]$. Of course $A[k]\in A$. Also, $A[k]\geq S_k[\text{len}(S_k)]$ so $S_{k+1}[\text{len}(S_{k+1})-1]\leq S_{k+1}[\text{len}(S_{k+1})]$. 

By the induction hypothesis, $S_k$ is a monotonically increasing subsequence of $A$. By showing that the new top element in $S_{k+1}$ is a member of $A$ and is no smaller than the old top element, we have shown that $S_{k+1}$ is also a monotonically increasing subsequence of $A$, and thus that the loop invariant holds.
\item Suppose instead $S_k$ was not empty and $S_k[\text{len}(S_k)]>A[k]$. Then the top element of the stack has been updated such that $S_{k+1}[\text{len}(S_{k+1})]=S_k[\text{len}(S_k)-1]$. By the induction hypothesis, $S_k[\text{len}(S_k)-1]\leq S_k[\text{len}(S_k)]$ and $S_k[\text{len}(S_k)-1]\in A$. By substituting and leveraging the strong induction hypothesis, we know that $S_{k+1}[\text{len}(S_{k+1}-1)]\leq S_{k+1}[\text{len}(S_{k+1})]$ and $S_{k+1}[\text{len}(S_{k+1})]\in A$.

Essentially, we have shown that by removing the top element of $S_k$ to produce $S_{k+1}$, we have returned to a prior state of $S$ for which the strong induction hypothesis asserts that the invariant holds.
\end{itemize}
\end{itemize}
In all cases, the loop invariant holds for $i=k+1$.

Hence, by the principle of mathematical induction, for all $1\leq i\leq\text{len}(A)+1$, we can conclude that $S_i$ is a monotonically increasing subsequence of $A$. 

By proving the loop invariant at initialization ($i=1$), maintenance ($1\leq i\leq\text{len}(A)$), and termination ($i=\text{len}(A)+1$), we have shown that $S$ is an increasing subsequence of $A$.$~\square$
\end{solution}
\item Prove that $S$ contains all elements of $A$ except at most $2\log_2 n$.
\begin{solution}\textbf{Invariant II.}

\textit{Claim. }Let $A$ be a close-to-sorted array such that at most $\log_2n$ elements are out of place. Let $S$ be an empty stack. Consider Algorithm 1.

We want to demonstrate that, for every iteration of the loop in Algorithm 1, the stack $S$ contains all elements of $A$ except at most $2\log_2n$. In other words, denoting the number of excluded elements $m$, we want to show that $m\leq 2\log_2n$.

Let $S_i$ denote $S$ before iteration $1\leq i\leq\text{len}(A)+1$. For example, $S_1$ represents the stack before iteration $1$ and $S_2$ represents the stack after iteration $1$ but before iteration $2$. Similarly, let $m_i$ denote the number of excluded elements of $A$ before iteration $i$.

We can prove this invariant by induction on $i$.

\textit{Basis. }Consider $i=1$ (before iteration 1). Here $S_1$ is an empty stack and $m_1=0$ since no elements have been examined or excluded. Of course, for all $n\in\mathbb{N}$, we have $0\leq 2\log_2n$, so $m_1\leq 2\log_2n$. The loop invariant holds in the base cases.

\textit{Hypothesis. }Consider $1<i<k\leq\text{len}(A)$ (before iteration $k$). Assume that $m_k\leq 2\log_2 n$.

\textit{Inductive step. }Consider $i=k+1$ (after iteration $k$, but before iteration $k+1$):
\begin{itemize}
\item Suppose $k+1=\text{len}(A)+1$. Then the loop terminated after iteration $k$. We have $m_{k+1}=m_k$, so, by the induction hypothesis, the loop invariant holds.
\item Suppose instead $k+1\leq\text{len}(A)$. Then the loop body executed after iteration $k$:
\begin{itemize}
\item Suppose $S_k$ was empty or $S_k[\text{len}(S_k)]\leq A[k]$. Note $S_k[\text{len}(S_k)]$ denotes the top element of the stack. No additional elements are excluded, so $m_{k+1}=m_k$. By the induction hypothesis, the loop invariant holds.
\item Suppose instead $S_k$ was not empty and $S_k[\text{len}(S_k)]>A[k]$. Then $A[k]$ is not pushed onto $S_{k+1}$, so one element is excluded. An element is popped from $S_k$, so another element is excluded. Considering these exclusions, $m_{k+1}=m_k+2$.

This case only occurs when $A[k]$ is out of place. Now, $A[k]$ is out of place for at most $\log_2n$ many iterations (since $A$ is almost sorted), so this case occurs at most $\log_2n$ many times. On each such iteration, exactly $2$ elements are excluded. Considering these bounds, taken together with the induction hypothesis ($m_k\leq 2\log_2n$), we can conclude that $m_{k+1}\leq 2\log_2n$ on any such iteration $k+1$. Thus the loop invariant holds.
\end{itemize}
\end{itemize}
In all cases, the loop invariant holds.

Hence, by the principle of mathematical induction, we conclude that the number of excluded elements $m\leq 2\log_2n$.
\end{solution}

    
    \item Use the previous statements to design and prove the correctness and run-time of an algorithm to sort the nearly sorted array in time $O(n)$. 
    
    \hint{Suppose you could split $A$ into a large sorted subsequence and only a few remaining unsorted elements in time $O(n)$. (This is what parts 3 and 4 show.) Now think of the ``Merge'' subroutine in Merge Sort.}

\begin{solution}   INSERT YOUR SOLUTION HERE   \end{solution}
\end{enumerate}





    



\subsection*{Honors Question: The Counterfeit Gem}
Suppose you strike up a relationship with a jeweler and get a bulk deal on $n$ diamonds. However, you were warned beforehand that one (and only one) of the diamonds is a convincing fake, and the only difference between it and the real gems is that the fake one's weight is different, while all real ones have the same weight. Luckily, you have a balance scale at home, which can test two (disjoint) subsets of the diamonds and tell us which is lighter. 

\begin{enumerate}
    \item $(**)$ Assume that you know that the fake one is lighter. Give an algorithm that finds the counterfeit gem in $O(\log n)$ uses of the scale. Justify the correctness and time complexity of your proposed algorithm.

\begin{solution}   INSERT YOUR SOLUTION HERE   \end{solution}

    \item $(****)$ Now consider the general case where you only know it has a different weight from the real ones. What can you do in this case? Can you still do it with $O(\log n)$ measurements? For concrete numbers, consider $n=12$ and show how you can identify the fake one only using the scale $3$ times.

\begin{solution}   INSERT YOUR SOLUTION HERE   \end{solution}
\end{enumerate}

