\lhead{\textbf{Basic Algorithms, Fall 2024 \\ CSCI-UA.0310-001}}
\chead{\Large{\textbf{Homework 3}}}
\def\lc{\left\lceil}   
\def\rc{\right\rceil}
\rhead{\textbf{Instructor: Rotem Oshman \\ Name: Ishan Pranav}}
\runningheadrule
\firstpageheadrule
\cfoot{}
\subsection*{References}
Collaborated with Crystal Huang. See \href{https://en.wikipedia.org/wiki/Exponentiation_by_squaring}{Exponentiation by squaring -- Wikipedia}.
\subsection*{Question 1: {\sc InsertionSort} \textit{vs.} {\sc QuickSort}}

Recall that in the worst case the running time of (non-randomized) {\sc QuickSort}, that uses the last element as the pivot, and {\sc InsertionSort} are $\Theta(n^2)$. Refer to the lecture notes for the {\sc QuickSort} implementation.

\begin{enumerate}
    \item Give an example of an array of length $n$ where both take time $\Omega(n^2)$. Justify your answer.  
\begin{solution}
Let $A[1,\dots,n]$ be an $n$-element array where $A[i]>A[i+1]$ for all $1\leq i<n$.\\

\textbf{Lemma I. }

\textit{Claim: }{\sc QuickSort}($A$) has running time $\Omega(n^2)$.

We will analyze each recursive call $i$ to {\sc QuickSort}, where $i\geq 1$.
\begin{itemize}
\item Consider $i=1$. This recursive call operates on $A$:
\begin{align*}
A&=(A[1],A[2],\dots,A[n]).
\end{align*}

Since the pivot is chosen to be the last element of $A$, and since $A$ is sorted in reverse order, the last element is the least element in $A$. In other words, the pivot is chosen to be $A[n]$ where $A[n]\leq A[i]$ for all $1<i<n$.

During the {\sc Partition} step, all elements are greater than or equal to $A[n]$, so no swaps are performed. Finally, $A[1]$ and $A[n]$ are swapped to place the pivot in its correct position. Note the resulting array $A_1$:
\begin{align*}
A_1&=(A[n],A[2],A[3]\dots,A[n-1],A[1]).
\end{align*}

\item Consider $i=2$. This recursive call operates on the subarray of $A_1$:
\begin{align*}
(A[2],A[3],\dots,A[n-1],A[1]).
\end{align*}
The pivot is chosen to be the last element of this subarray, which is the value $A[1]$. Because of the previous swap, in this case $A[1]$ is the largest element in the subarray. 

During the {\sc Partition} step, all elements in the subarray are less than or equal to $A[1]$, so no swaps are performed. Finally, the values $A[n-1]$ and $A[1]$ are swapped to place the pivot in its correct position. Note the resulting array $A_2$:
\begin{align*}
A_2=(A[2],A[3],\dots,A[1],A[n-1]).
\end{align*}
\end{itemize}

For each odd recursive step $i=2k+1$, the pivot is the least element; for each even recursive step $i=2k$, the pivot is the greatest element. Thus, for each recursive step, the pivot is an extreme within its working subarray. As a result, the {\sc Partition} step results in a lopsided set of subarrays, with one of length $n-1$ and the other of length $0$.

On every recursive step, the size of the current working subarray is reduced by exactly 1. Therefore, there are $n$ levels, and on each level $i$, there are $n-i$ operations performed. Thus the amount of work is no less than
\begin{equation*}
\sum_{i=1}^n(n-i)=n+(n-1)+(n-2)+\dots=\Omega(n).
\end{equation*}

Therefore {\sc QuickSort}($A$) has running time $\Omega(n^2)$.\\

\textbf{Lemma II.}

\textit{Claim. }{\sc InsertionSort}($A$) has running time $\Omega(n^2)$.

We will analyze each iteration $i$ of {\sc InsertionSort}, where $i\geq 1$.

\begin{itemize}
\item Consider $i=1$. This iteration operates on $A[2]$. Note $A[2]<A[1]$, so a comparison and a shift is required.
\item Consider $i=2$. This iteration operates on $A[3]$. Note $A[3]<A[2]$ and $A[3]<A[1]$, so two comparisons and two shifts are required.
\end{itemize}
For each iteration $i$, there are $i$-many comparisons and shifts required to insert $A[i+1]$ into its sorted position. Thus the amount of work is no less than
\begin{equation*}
\sum_{i=1}^n{i}=1+2+\dots+n=\Omega(n^2).
\end{equation*}
Therefore {\sc InsertionSort}($A$) has running time $\Omega(n^2)$.\\

\textbf{Proof. }

From Lemma I, We have that {\sc QuickSort}($A$) has running time $\Omega(n^2)$.

From Lemma II, We have that {\sc InsertionSort}($A$) has running time $\Omega(n^2)$.

Hence proven.$~\square$
\end{solution}
\newpage
\item Give an example of an array of length $n$ where (non-randomized) {\sc QuickSort} runs in $\Omega(n^2)$ but {\sc Insertion-Sort} runs in time $O(n)$. Justify your answer. 
\begin{solution}
Let $A[1,\dots,n]$ be an $n$-element array where $A[i]<A[i+1]$ for all $1\leq i<n$.\\

\textbf{Lemma I. }

\textit{Claim: }{\sc QuickSort}($A$) has running time $\Omega(n^2)$.

We will analyze each recursive call $i$ to {\sc QuickSort}, where $i\geq 1$.
\begin{itemize}
\item Consider $i=1$. This recursive call operates on $A$:
\begin{align*}
A&=(A[1],A[2],\dots,A[n]).
\end{align*}

Since the pivot is chosen to be the last element of $A$, and since $A$ is sorted, the last element is the greatest element in $A$. In other words, the pivot is chosen to be $A[n]$ where $A[n]\geq A[i]$ for all $1<i<n$.

During the {\sc Partition} step, all elements are less than or equal to $A[n]$, so no swaps are performed. The pivot $A[n]$ remains in its correct position. There is no change to the array.

\item Consider $i=2$. This recursive call operates on the subarray of $A$:
\begin{align*}
(A[1],A[2],\dots,A[n-1]).
\end{align*}
Again, the pivot is chosen to be the last element of this subarray, which is the value $A[n-1]$, the largest element in this subarray of $A$.

During the {\sc Partition} step, all elements in the subarray are less than or equal to $A[n-1]$, so no swaps are performed. Again, the pivot $A[n-1]$ remains in its correct position. Again, there is no change to the array.
\end{itemize}

For each recursive step $i$, the pivot is the greatest element. Thus, for each recursive step, the pivot is the maximum within its working subarray. As a result, the {\sc Partition} step results in a lopsided set of subarrays, with one of length $n-1$ and the other of length $0$.

On every recursive step, the size of the current working subarray is reduced by exactly 1. Therefore, there are $n$ levels, and on each level $i$, there are $n-i$ operations performed. Thus the amount of work is no less than
\begin{equation*}
\sum_{i=1}^n(n-i)=n+(n-1)+(n-2)+\dots=\Omega(n).
\end{equation*}

Therefore {\sc QuickSort}($A$) has running time $\Omega(n^2)$.\\

\textbf{Lemma II.}

\textit{Claim. }{\sc InsertionSort}($A$) has running time $\Omega(n^2)$.

We will analyze each iteration $i$ of {\sc InsertionSort}, where $i\geq 1$.

\begin{itemize}
\item Consider $i=1$. This iteration operates on $A[2]$. Note $A[2]>A[1]$, so only a comparison is required.
\item Consider $i=2$. This iteration operates on $A[3]$. Note $A[3]>A[2]$, so only a comparison is required.
\end{itemize}
For each iteration $i$, there is 1 comparison required to ensure $A[i+1]$ is in its sorted position. Let $c$ be the positive constant that denotes the running time of each comparison operation. Thus the amount of work is no less than
\begin{equation*}
\sum_{i=1}^n{c}=cn=\Omega(n).
\end{equation*}
Therefore {\sc InsertionSort}($A$) has running time $\Omega(n)$.\\

\textbf{Proof. }

From Lemma I, We have that {\sc QuickSort}($A$) has running time $\Omega(n^2)$.

From Lemma II, We have that {\sc InsertionSort}($A$) has running time $\Omega(n)$.

Hence proven.$~\square$
\end{solution}
\end{enumerate}
\newpage
\subsection*{Question 2: Fast exponentiation}
In this problem, you will design an algorithm to compute $2024^n$ given $n \in \mathbb{N}$ as input. In each case, prove the correctness of your algorithm, and an upper bound on the number of multiplications used.
\begin{enumerate}
    \item Using $n-1$ many multiplications.
\begin{solution}
{\sc NaiveExponentiation}($n$) where $n\in\mathbb{N}$:
\begin{itemize}
\item if $n=1$, then return $2024$;
\item otherwise, return the product $2024~\times~${\sc NaiveExponentiation}($n-1$).
\end{itemize}
\textbf{Lemma I. }

\textit{Claim. }{\sc NaiveExponentiation}($n$)$~=2024^n$ for all $n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. We have {\sc NaiveExponentiation}($1$)$~=2024=2024^1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=k$ where $k>1$. Assume that {\sc NaiveExponentiation}($k$)$~=2024^k$.

\textit{Inductive step. }Consider $n=k+1$. Then

{\sc NaiveExponentiation}($k+1$)$~=2024~\times~${\sc NaiveExponentiation}($(k+1)-1$). That is,

{\sc NaiveExponentiation}($k+1$)$~=2024~\times~${\sc NaiveExponentiation}($k$).

By the inductive hypothesis, we have {\sc NaiveExponentiation}($k$)$~=2024^k$. Thus

{\sc NaiveExponentiation}($k+1$)$~=2024\times2024^k$. Ergo,

{\sc NaiveExponentiation}($k+1$)$~=2024^{k+1}$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc NaiveExponentiation} algorithm produces the correct result for all $n\in\mathbb{N}$.\\

\textbf{Lemma II. }

\textit{Claim. }{\sc NaiveExponentiation}($n$) uses at most $n-1$ multiplications for all $n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. Since $n=1$, the number of multiplications is $0=1-1\leq n-1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=k$ where $k>1$. Assume that {\sc NaiveExponentiation}($k$) uses at most $k-1$ multiplications.

\textit{Inductive step. }Consider $n=k+1$. Then the number of multiplications is $1$, plus the number of multiplications used by {\sc NaiveExponentiation}($(k+1)-1$). That is, $1$ plus the number of multiplications used by {\sc NaiveExponentiation}($k$). By the inductive hypothesis, the number of multiplications used by {\sc NaiveExponentiation}($k$) is at most $k-1$. Therefore, the number of multiplications used by {\sc NaiveExponentiation}($k+1$) is at most $1+(k-1)=(k+1)-1$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc NaiveExponentiation} algorithm uses at most $n-1$ multiplications for all $n\in\mathbb{N}$.\\

\textbf{Proof. }

By Lemma I, {\sc NaiveExponentiation}($n$) computes $2024^n$ for all $n\in\mathbb{N}$.

By Lemma II, {\sc NaiveExponentiation}($n$) uses at most $n-1$ multiplications for all $n\in\mathbb{N}$.

Ergo, for all $n\in\mathbb{N}$, we have demonstrated that {\sc NaiveExponentiation}($n$) computes $2024^n$ using at most $n-1$ multiplications.$~\square$
\end{solution}
\newpage
    \item Using $O(\log_2 n)$ many multiplications, assuming $n$ is a power of $2$, i.e., $n=2^k$.
\begin{solution}
{\sc SquareExponentiation}($n$) where $n=2^x$ for $x\geq 0$:
\begin{itemize}
\item if $n=2^0$, then return $2024$;
\item otherwise, compute $y\leftarrow~${\sc SquareExponentiation}($n/2$), and return $y\times y$.
\end{itemize}
\textbf{Lemma I. }

\textit{Claim. }{\sc SquareExponentiation}($n$)$~=2024^n$ for $n=2^x$ where $x\geq 0$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=2^0$. We have {\sc SquareExponentiation}($2^0$)$~=2024=2024^1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=2^k$ where $k>0$. Assume that {\sc SquareExponentiation}($2^k$)$~=2024^{2^k}$.

\textit{Inductive step. }Consider $n=2^{k+1}$. Then

$y=~${\sc SquareExponentiation}($2^{k+1}/2$)$~=~${\sc SquareExponentiation}($2^k$). By the inductive hypothesis, we have
{\sc SquareExponentiation}($2^k$)$~=~2024^{2^k}$. Thus $y=2024^{2^k}$. Now

{\sc SquareExponentiation}($2^{k+1}$)$~=~y\times y$, so {\sc SquareExponentiation}($2^{k+1}$)$~=2024^{2^k}\times 2024^{2^k}=2024^{2^k+2^k}=2024^{2\cdot 2^k}=2024^{2^{k+1}}$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc SquareExponentiation} algorithm produces the correct result for all $n=2^x$ where $x\geq 0$.\\

\textbf{Lemma II. }

\textit{Claim. }{\sc SquareExponentiation}($n$) uses $O(\log_2n)$ multiplications for $n=2^x$ where $x\geq 0$. Let $M(n)$ denote the number of multiplications used by {\sc Square Exponentiation}($n$). We claim that $M(n)=O(\log_2n)$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=2^0$. Then there are no multiplications, so $M(2^0)=0$. Since $\log_2n\geq 0$ for $n\geq 1$, we can choose any positive constant $C$ and any $n_0\geq 1$ such that $0\leq C\log_2n$ for $n>n_0$. Therefore $M(n)=O(\log_2n)$ holds in the base case.

\textit{Hypothesis. }Consider $n=2^k$ where $k>0$. Assume that $M(2^k)=O(\log_2k)$.

\textit{Inductive step. }Consider $n=2^{k+1}$. Then the number of multiplications is $1$ (to compute $y\times y$), plus the number of multiplications used by the recursive call. That is, 
\begin{align*}
M(2^{k+1})&=1+M\left(\frac{2^{k+1}}{2}\right)\\
&=1+M(2^k).
\end{align*}

By the inductive hypothesis, we have $M(2^k)=O(\log_2(k+1))$. Asymptotically,
\begin{align*}
M(2^{k+1})&=1+O(\log_2(k+1))\\
&=O(1)+O(\log_2k)\\
&=O(\log_2k).
\end{align*}

This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc SquareExponentiation} algorithm uses $O(\log_2n)$ many multiplications for all $n=2^x$ where $x\geq 0$.\\

\textbf{Proof. }

By Lemma I, {\sc SquareExponentiation}($n$) computes $2024^n$ for all $n=2^x$ where $x\geq 0$.

By Lemma II, {\sc SquareExponentiation}($n$) uses at $O(\log_2n)$ many multiplications for all $n=2^x$ where $x\geq 0$.

Ergo, for all $n=2^x$ where $x\geq 0$, we have demonstrated that {\sc SquareExponentiation}($n$) computes $2024^n$ using $O(\log_2n)$ many multiplications.$~\square$
\end{solution}
\newpage
\item Using $O(\log_2 n)$ many multiplications for \emph{any} $n$ (not necessarily a power of $2$).
\begin{solution}\\

\textbf{Algorithm I. }{\sc FastExponentiation}($b,n$) where $n\in\mathbb{N}$:
\begin{itemize}
\item if $n=1$, then return $b$;
\item otherwise, compute $b'\leftarrow b\times b$, and:
\begin{itemize}
    \item if $n$ is even, then return {\sc FastExponentiation}($b',n/2$);
    \item otherwise, return the product $b~\times~${\sc FastExponentiation}($b',(n-1)/2$).
\end{itemize}
\end{itemize}

\textbf{Algorithm II. }{\sc FastPow2024($n$)} where $n\in\mathbb{N}$: return the result {\sc FastExponentiation}($2024,n$).\\

\textbf{Lemma I.}

\textit{Claim. }{\sc FastExponentiation}($b,n$)$~=b^n$ for all $b,n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. We have {\sc FastExponentiation}($b,1$)$~=b=b^1$ for all $b\in\mathbb{N}$. The claim holds in the base case.

\textit{Hypothesis. }Consider $1<n\leq k$. Assume that {\sc FastExponentiation}($b,k$)$~=b^k$ for all $b\in\mathbb{N}$.

\textit{Inductive step. }Consider $n=k+1$. Since $k+1>1$, we compute $b'=b\times b$. Of course, since $b\in\mathbb{N}$, we have $b'\in\mathbb{N}$. Since $k\in\mathbb{N}$, either $k+1$ is even or $k+1$ is odd:
\begin{itemize}
\item Suppose $k+1$ is even. Now,

{\sc FastExponentiation}($b,k+1$)$~=~${\sc FastExponentiation}($b',\frac{k+1}{2}$).

Since $k+1\in\mathbb{N}$ and $k+1$ is even, we have $\frac{k+1}{2}\in\mathbb{N}$. By the strong induction hypothesis, {\sc FastExponentiation}($b',\frac{k+1}{2}$)$~={b'}^{\frac{k+1}{2}}=(b\times b)^{\frac{k+1}{2}}=(b^2)^{\frac{k+1}{2}}=b^{k+1}$. Ergo

{\sc FastExponentiation}($b,k+1$)$~=b^{k+1}$.
\item Suppose instead $k+1$ is odd. Now,

{\sc FastExponentiation}($b,k+1$)$~=b~\times~${\sc FastExponentiation}($b',\frac{(k+1)-1}{2}$).

Since $k+1\in\mathbb{N}$ and $k+1$ is odd, we have $\frac{(k+1)-1}{2}\in\mathbb{N}$. By the strong induction hypothesis, {\sc FastExponentiation}($b',\frac{(k+1)-1}{2}$)$~={b'}^{\frac{(k+1)-1}{2}}={b'}^{\frac{k}{2}}=(b\times b)^{\frac{k}{2}}=(b^2)^{\frac{k}{2}}=b^k$. Ergo

{\sc FastExponentiation}($b,k+1$)$~=b\times b^k=b^{k+1}$.
\end{itemize}

In all cases, {\sc FastExponentiation}($b,k+1$)$~=b^{k+1}$ for all $b\in\mathbb{N}$. This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc FastExponentiation} algorithm produces the correct result for all $b,n\in\mathbb{N}$.\\

\textbf{Lemma II.}

\textit{Claim. }{\sc FastExponentiation}($b,n$)$~=b^n$ uses $O(\log_2n)$ multiplications for all $b,n\in\mathbb{N}$. Let $M(b,n)$ denote the number of multiplications used by {\sc FastExponentiation}($b,n$). We claim that $M(b,n)=O(\log_2n)$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. Then, regardless of the base $b$, there are no multiplications, so $M(b,1)=0$ for all $b\in\mathbb{N}$. Since $\log_2n\geq 0$ for $n\geq 1$, we can choose any positive constant $C$ and any $n_0\geq 1$ such that $0\leq C\log_2n$ for $n\geq n_0$. Therefore $M(b,n)=O(\log_2n)$ holds for all $b\in\mathbb{N}$ in the base case.

\textit{Hypothesis. }Consider $1<n\leq k$. Assume that $M(b,k)=O(\log_2 n)$ for all $b\in\mathbb{N}$.

\textit{Inductive step. }Consider $n=k+1$. Since $k+1>1$, we know $1$ multiplication is required to compute $b'=b\times b$. Since $k\in\mathbb{N}$, either $k+1$ is even or $k+1$ is odd:
\begin{itemize}
\item Suppose $k+1$ is even. The number of additional multiplications is $M\left(b',\frac{k+1}{2}\right)$. By the strong induction hypothesis, we have $M\left(b',\frac{k+1}{2}\right)=O(\log_2n)$.
\item Suppose instead $k+1$ is odd. The number of additional multiplications is $M\left(b',\frac{(k+1)-1}{2}\right)$. By the strong induction hypothesis, we have $M\left(b',\frac{(k+1)-1}{2}\right)=M\left(b',\frac{k}{2}\right)=O(\log_2n)$.
\end{itemize}
In all cases, the number of additional multiplications is $O(\log_2n)$. Considering also the single multiplication used to compute $b'=b\times b$, asymptotically we have
\begin{align*}
M(b,k+1)&=1+O(\log_2n)\\
&=O(1)+O(\log_2n)\\
&=O(\log_2n).
\end{align*}
This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc FastExponentiation} algorithm uses $O(\log_2n)$ many multiplications for all $b,n\in\mathbb{N}$.\\

\textbf{Proof.}

By construction, the {\sc FastPow2024}($n$) algorithm returns {\sc FastExponentiation}($2024,n$) and uses exactly as many multiplications as the {\sc FastExponentiation}($2024,n$) algorithm.

By Lemma I, {\sc FastExponentiation}($2024,n$) computes $2024^n$ for all $n\in\mathbb{N}$. Therefore, the {\sc FastPow2024} algorithm computes $2024^n$ for all $n\in\mathbb{N}$.

By Lemma II, {\sc FastExponentiation}($2024,n$) uses $O(\log_2n)$ many multiplications for all $n\in\mathbb{N}$. Therefore, the {\sc FastPow2024} algorithm uses $O(\log_2n)$ many multiplications for all $n\in\mathbb{N}$.

Ergo, for all $n\in\mathbb{N}$, we have demonstrated that {\sc FastPow2024}($n$) computes $2024^n$ using $O(\log_2n)$ many multiplications.$~\square$
\end{solution}
\newpage
\end{enumerate}
\subsection*{Question 3: Close to sorted}
Suppose you are given a list $A$ of $n$ distinct numbers. You are guaranteed that this list is `close to sorted' in the following sense: if $A_{\text{sorted}}$ denotes the list fully sorted in increasing order, then $A_{\text{sorted}}$ differs from $A$ in at most $\log_2 n$ many positions. Intuitively, this says that at most $\log_2 n$ elements are out of place in the original list. For instance, in the list 
\[
(2,5,9,11,20,14,15,12,25,30)
\]
only two elements are out of place (20 and 12), since after sorting, we get 
\[
(2,5,9,11,12,14,15,20,25,30) \;.
\]
The following exercises will ultimately lead to an algorithm which sorts $A$ in time $O(n)$~\footnote{Note that sorting algorithms take $O(n\log n)$ time without any assumptions on the inputs. Here, we are able to get the faster $O(n)$ run-time by \emph{assuming} that the input array $A$ is already close to being sorted.}. Answer each exercise.

\begin{enumerate}
    \item Prove that the leftmost (first) out of place element must be too big (not too small) for its place.
\begin{solution}
\textit{Claim. }Let $k$ represent the position of the leftmost out-of-place element in $A$. Then $A[k]$ is too big (not too small) for its position.

\textit{Proof. }Assume, for the sake of contradiction, that $k$ is too small for its position. Since $A[k]$ is too small for its position, we know $k>1$. Otherwise, if $k=1$, it could not be too small for its position. Of course, since $A[k]$ is too small for its position, $A[k]<A[k-1]$.

This implies that $A[k-1]>A[k]$, meaning that $A[k-1]$ is too big for its place. $A[k-1]$ is out of place and $k-1<k$. Therefore, $A[k-1]$ is out of place and \textit{further left} than $A[k]$. This contradicts the hypothesis that $A[k]$ is the leftmost out-of-place element in $A$.

We conclude that the leftmost out-of-place element in $A$ must \textit{not} be too small for its place. Hence, the leftmost out-of-place element in $A$ must be too big for its place.$~\square$
\end{solution}
\newpage
    \item Suppose we construct a stack $S$ as follows: We go through $A$ from left to right, pushing elements one by one into $S$ as long as the next element from $A$ is larger than the top element $s$ of the stack so far. If at any step $i$ the element $A[i]$ which we are currently considering in $A$ is smaller than $s$, we instead pop $s$ from $S$ and continue.
    This idea is described in the pseudo code below:
    \begin{minipage}{\linewidth}
    \begin{algorithm}[H]
    \label{incsubseq}
    \caption{Construct increasing subsequence $S$}
    \begin{algorithmic}
    \REQUIRE $A$ and an empty stack $S$.
    \ENSURE $S$ representing an increasing subsequence of $A$
    \STATE $n \leftarrow \text{len}(A)$
    \FOR {$i = 1 \text{ to } n$}
        \IF {$S$ is empty \OR $S.\text{topElement}() \leq A[i]$}
        \STATE $S.\text{push}(A[i])$
        \ELSE 
        \STATE $S.\text{pop}()$
        \ENDIF
    \ENDFOR
    \STATE \RETURN $S$
    \end{algorithmic}
    \end{algorithm}
    \end{minipage}
    \smallskip
    
    If $A$ is initially $(2,5,9,11,20,14,15,12,25,30)$, what will $S$ be after Algorithm 1 has run?
\begin{solution}
Let $A[1,\dots,n]=(2,5,9,11,20,14,15,12,25,30)$. Let $S$ be an empty stack. 

Let $S_i$ denote $S$ before iteration $1\leq i\leq n+1$. For example, $S_1$ represents the stack before iteration $1$ and $S_2$ represents the stack after iteration $1$ but before iteration $2$. $S_{n+1}$ represents $S$ after the final iteration. Stepping through the algorithm:
\begin{align*}
S_1&=()&\textit{before the first iteration.}\\
S_2&=(2)&\textit{since $S_1$ is empty.}\\
S_3&=(2,5)&\textit{since $2\leq 5$.}\\
~&\vdots&\vdots\\
S_6&=(2,5,9,11,20)&\textit{since $5\leq 9\leq 11\leq 20$.}\\
S_7&=(2,5,9,11)&\textit{since $S$ is not empty, $20\nleq 14$.}\\
S_8&=(2,5,9,11,15)&\textit{since $11\leq 15$.}\\
S_9&=(2,5,9,11)&\textit{since $S$ is not empty, $15\nleq 12$.}\\
~&\vdots&\vdots\\
S_{11}&=(2,5,9,11,25,30)&\textit{since $11\leq 25\leq 30$.}
\end{align*}
After Algorithm 1 has run, $S=(2,5,9,11,25,30)$.
\end{solution}
\newpage\item Prove that $S$ is an increasing subsequence of $A$ (the elements of the output stack are increasing from bottom to top).
\begin{solution}\textbf{Invariant I.}

\textit{Claim. }Let $A[1,\dots,n]$ be an array and $S$ be an empty stack. Consider Algorithm 1.

Let $S_i$ denote $S$ before iteration $1\leq i\leq n+1$. For example, $S_1$ represents the stack before iteration $1$ and $S_2$ represents the stack after iteration $1$ but before iteration $2$. 

We want to demonstrate that, for every iteration of the loop in Algorithm 1, the stack $S$ is a monotonically increasing subsequence of $A$.

That is, for all $i\leq n+1$, for all $s\in S_i$, we have $s\in A$, and for all $1\leq j\leq\text{len}(S_i)$, we have $S_i[j]\leq S_i[j+1]$.

We can prove this invariant by induction on $i$.

\textit{Basis. }Consider $i=1$ (before iteration $1$). Here $S_1$ is an empty stack. Vacuously, for all $s\in S_1$, we have $s\in A$. Similarly, for all $1\leq j<\text{len}(S_1)$, we have $S_1[j]\leq S_1[j+1]$. The invariant holds in the base case.

\textit{Hypothesis. }Consider $1<i\leq k\leq\text{len}(A)$ (before iteration $k$). Assume that for all $s\in S_k$, we have $s\in A$, and for all $1\leq j\leq\text{len}(S_k)$, we have $S_k[j]\leq S_k[j+1]$.

\textit{Inductive step. }Consider $i=k+1$ (after iteration $k$, but before iteration $k+1$):
\begin{itemize}
\item Suppose $S_k$ was empty or $S_k[\text{len}(S_k)]\leq A[k]$. Note $S_k[\text{len}(S_k)]$ denotes the top element of the stack.

Then the top element of the stack has been updated such that $S_{k+1}[\text{len}(S_{k+1})]=A[k]$.

Of course $A[k]\in A$.

Also, $A[k]\geq S_k[\text{len}(S_k)]$ so $S_{k+1}[\text{len}(S_{k+1})-1]\leq S_{k+1}[\text{len}(S_{k+1})]$. 

By the induction hypothesis, $S_k$ is a monotonically increasing subsequence of $A$. By showing that the new top element in $S_{k+1}$ is a member of $A$ and is no smaller than the old top element, we have shown that $S_{k+1}$ is also a monotonically increasing subsequence of $A$, and thus that the loop invariant holds.
\item Suppose instead $S_k$ was not empty and $S_k[\text{len}(S_k)]>A[k]$. Then the top element of the stack has been updated such that $S_{k+1}[\text{len}(S_{k+1})]=S_k[\text{len}(S_k)-1]$. By the induction hypothesis, $S_k[\text{len}(S_k)-1]\leq S_k[\text{len}(S_k)]$ and $S_k[\text{len}(S_k)-1]\in A$. By substituting and leveraging the strong induction hypothesis, we know that $S_{k+1}[\text{len}(S_{k+1}-1)]\leq S_{k+1}[\text{len}(S_{k+1})]$ and $S_{k+1}[\text{len}(S_{k+1})]\in A$.

Essentially, we have shown that by removing the top element of $S_k$ to produce $S_{k+1}$, we have returned to a prior state of $S$ for which the strong induction hypothesis asserts that the invariant holds.
\end{itemize}
In all cases, the loop invariant holds for $i=k+1$.

Hence, by the principle of mathematical induction, for all $1\leq i\leq n+1$, we can conclude that $S_i$ is a monotonically increasing subsequence of $A$. 

By proving the loop invariant at initialization ($i=1$), maintenance ($1\leq i\leq n$), and termination ($i=n+1$), we have shown that $S$ is an increasing subsequence of $A$.$~\square$
\end{solution}
\newpage
\item Prove that $S$ contains all elements of $A$ except at most $2\log_2 n$.
\begin{solution}\textbf{Invariant II.}

\textit{Claim. }Let $A[1,\dots,n]$ be a close-to-sorted array such that at most $\log_2n$ elements are out of place. Let $S$ be an empty stack. Consider Algorithm 1.

We want to demonstrate that, for every iteration of the loop in Algorithm 1, the stack $S$ contains all elements of $A$ except at most $2\log_2n$. In other words, denoting the number of excluded elements $m$, we want to show that $m\leq 2\log_2n$.

Let $S_i$ denote $S$ before iteration $1\leq i\leq n+1$. For example, $S_1$ represents the stack before iteration $1$ and $S_2$ represents the stack after iteration $1$ but before iteration $2$. Similarly, let $m_i$ denote the number of excluded elements of $A$ before iteration $i$.

We can prove this invariant by induction on $i$.

\textit{Basis. }Consider $i=1$ (before iteration 1). Here $S_1$ is an empty stack and $m_1=0$ since no elements have been examined or excluded. Of course, for all $n\in\mathbb{N}$, we have $0\leq 2\log_2n$, so $m_1\leq 2\log_2n$. The loop invariant holds in the base cases.

\textit{Hypothesis. }Consider $1<i<k\leq n$ (before iteration $k$). Assume that $m_k\leq 2\log_2 n$.

\textit{Inductive step. }Consider $i=k+1$ (after iteration $k$, but before iteration $k+1$):
\begin{itemize}
\item Suppose $S_k$ was empty or $S_k[\text{len}(S_k)]\leq A[k]$. Note $S_k[\text{len}(S_k)]$ denotes the top element of the stack. No additional elements are excluded, so $m_{k+1}=m_k$. By the induction hypothesis, the loop invariant holds.
\item Suppose instead $S_k$ was not empty and $S_k[\text{len}(S_k)]>A[k]$. Then $A[k]$ is not pushed onto $S_{k+1}$, so one element is excluded. An element is popped from $S_k$, so another element is excluded. Considering these exclusions, $m_{k+1}=m_k+2$.

This case only occurs when $A[k]$ is out of place. Now, $A[k]$ is out of place for at most $\log_2n$ many iterations (since $A$ is almost sorted), so this case occurs at most $\log_2n$ many times. On each such iteration, exactly $2$ elements are excluded. Considering these bounds, taken together with the induction hypothesis ($m_k\leq 2\log_2n$), we can conclude that $m_{k+1}\leq 2\log_2n$ on any such iteration $k+1$. Thus the loop invariant holds.
\end{itemize}
In all cases, the loop invariant holds.

Hence, by the principle of mathematical induction, we conclude that the number of excluded elements $m\leq 2\log_2n$.$~\square$
\end{solution}
\newpage\item Use the previous statements to design and prove the correctness and run-time of an algorithm to sort the nearly sorted array in time $O(n)$.
\begin{solution}{\sc CloseSort}($A$) where $A[1,\dots,n]$ is a close-to-sorted array such that at most $\log_2n$ elements are out of place:

Let $S$ be an empty stack. Perform Algorithm 1 using $A$ and $S$, storing each of the $\ell$ pairs of elements $(a,b)$ excluded from $S$ in $\ell$-many $2$-element arrays $(a_1,b_1),(a_2,b_2),\dots,(a_\ell,b_\ell)$ such that $a_i<b_i$ for all $1\leq i\leq\ell$. 

Return $A'\leftarrow~${\sc Merge}($S$,$(a_1,b_1),(a_2,b_2),\dots,(a_\ell,b_\ell)$). Here {\sc Merge}($A_1,A_2,\dots,A_k$) is the well-known algorithm to merge $k$-many monotonically increasing sequences into a single sorted array. Note that the running time of {\sc Merge} is known to be $O(n)$, where $n$ is the sum of the lengths of the $k$ arrays to merge. Intuitively, this is because the algorithm involves scanning each sequences and copying values from the source sequences into the destination array.\\

\textbf{Lemma I.}

\textit{Claim. }{\sc CloseSort}($A$) returns an array $A'$ containing the elements of $A$ in sorted order.

\textit{Proof. }
From Invariant I, we have that $S$ is a monotonically increasing subsequence of $A$.

By construction, each of $(a_i,b_i)$ for $1\leq i\leq\ell$ is monotonically increasing.

From the {\sc Merge} invariant, $A'$ contains the elements of $S$ and $(a_1,b_1),(a_2,b_2),\dots,(a_\ell,b_\ell)$ in sorted order.

By construction, $S$ is the result of Algorithm 1 and the arrays $(a_1,b_1),(a_2,b_2),\dots,(a_\ell,b_\ell)$ contain all 
elements in $A$ not included in $S$, and only those elements in $A$ not included in $S$. Therefore $A'$ contains all elements in $A$, and only elements in $A$. That is, for all $a\in A$, we have $a\in A'$ and for all $a'\in A'$, we have $a'\in A$.

Hence, {\sc CloseSort} returns an array $A'$ containing the elements of $A$ in sorted order.\\

\textbf{Lemma II.}

\textit{Claim. }The running time of {\sc CloseSort}($A$) is $O(n)$.

\textit{Proof. }Let $T(n)$ denote the running time of {\sc CloseSort}($A$). From Invariant II, we have that the number of excluded elements $m\leq 2\log_2n$.

Therefore, there are at most $\log_2n$ pairs of excluded elements. We have $\ell\leq\log_2n$.

The running time of {\sc CloseSort} is the sum of the running times of Algorithm 1, $T_1(n)$, and {\sc Merge}, $T_2(n)$, plus some constant overhead $C$. The running times of Algorithm 1 and {\sc Merge} are given to be linear with respect to their inputs.
\begin{align*}
T(n)&=T_1(n)+T_2(n)+C&~\\
&=O(n)+O(\text{len}(S)+2\ell)+C&\textit{considering the number of elements to merge,}\\
&=O(n)+O(n)+C&\textit{$\text{len}(S)+2\ell=n$ by construction,}\\
&=O(n)&\textit{asymptotically.}
\end{align*}

Hence, the running time of {\sc CloseSort} on an $n$-element almost-sorted array is $O(n)$.

\textbf{Proof.}

Let $A[1,\dots n]$ be a close-to-sorted array such that at most $\log_2n$ elements are out of place.

From Lemma I, {\sc CloseSort}($A$) returns $A'$, the sorted version of $A$. 

From Lemma II, the running time of {\sc CloseSort}($A$) is $O(n)$.

Ergo, {\sc CloseSort} sorts a close-to-sorted array in linear time.$~\square$
\end{solution}
\end{enumerate}
