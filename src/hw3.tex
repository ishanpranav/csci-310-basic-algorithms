\lhead{\textbf{Basic Algorithms, Fall 2024 \\ CSCI-UA.0310-001}}
\chead{\Large{\textbf{Homework 3}}}
\def\lc{\left\lceil}   
\def\rc{\right\rceil}
\rhead{\textbf{Instructor: Rotem Oshman \\ Name: Ishan Pranav}}
\runningheadrule
\firstpageheadrule
\cfoot{}
\subsection*{References}
Collaborated with Crystal Huang.
\subsection*{Question 1: {\sc InsertionSort} \textit{vs.} {\sc QuickSort}}

Recall that in the worst case the running time of (non-randomized) {\sc QuickSort}, that uses the last element as the pivot, and {\sc InsertionSort} are $\Theta(n^2)$. Refer to the lecture notes for the {\sc QuickSort} implementation.

\begin{enumerate}
    \item Give an example of an array of length $n$ where both take time $\Omega(n^2)$. Justify your answer.  
\begin{solution}
Let $A[1,\dots n]$ be an $n$-element array where $A[i]\geq A[i+1]$ for all $1\leq i<n$.
\end{solution}
    \item Give an example of an array of length $n$ where (non-randomized) {\sc QuickSort} runs in $\Omega(n^2)$ but {\sc Insertion-Sort} runs in time $O(n)$. Justify your answer. 
\begin{solution}
Let $A[1,\dots n]$ be an $n$-element array where $A[i]\leq A[i+1]$ for all $1\leq i<n$.
\end{solution}
\end{enumerate}
\newpage
\subsection*{Question 2: Fast exponentiation}
In this problem, you will design an algorithm to compute $2024^n$ given $n \in \mathbb{N}$ as input. In each case, prove the correctness of your algorithm, and an upper bound on the number of multiplications used.
\begin{enumerate}
    \item Using $n-1$ many multiplications.
\begin{solution}
{\sc NaiveExponentiation}($n$) where $n\in\mathbb{N}$:
\begin{itemize}
\item if $n=1$, then return $2024$;
\item otherwise, return the product $2024~\times~${\sc NaiveExponentiation}($n-1$).
\end{itemize}
\textbf{Lemma I. }

\textit{Claim. }{\sc NaiveExponentiation}($n$)$~=2024^n$ for all $n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. We have {\sc NaiveExponentiation}($1$)$~=2024=2024^1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=k$ where $k>1$. Assume that {\sc NaiveExponentiation}($k$)$~=2024^k$.

\textit{Inductive step. }Consider $n=k+1$. Then

{\sc NaiveExponentiation}($k+1$)$~=2024~\times~${\sc NaiveExponentiation}($(k+1)-1$). That is,

{\sc NaiveExponentiation}($k+1$)$~=2024~\times~${\sc NaiveExponentiation}($k$).

By the inductive hypothesis, we have {\sc NaiveExponentiation}($k$)$~=2024^k$. Thus

{\sc NaiveExponentiation}($k+1$)$~=2024\times2024^k$. Ergo,

{\sc NaiveExponentiation}($k+1$)$~=2024^{k+1}$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc NaiveExponentiation} algorithm produces the correct result for all $n\in\mathbb{N}$.\\

\textbf{Lemma II. }

\textit{Claim. }{\sc NaiveExponentiation}($n$) uses at most $n-1$ multiplications for all $n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. Since $n=1$, the number of multiplications is $0=1-1\leq n-1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=k$ where $k>1$. Assume that {\sc NaiveExponentiation}($k$) uses at most $k-1$ multiplications.

\textit{Inductive step. }Consider $n=k+1$. Then the number of multiplications is $1$, plus the number of multiplications used by {\sc NaiveExponentiation}($(k+1)-1$). That is, $1$ plus the number of multiplications used by {\sc NaiveExponentiation}($k$). By the inductive hypothesis, the number of multiplications used by {\sc NaiveExponentiation}($k$) is at most $k-1$. Therefore, the number of multiplications used by {\sc NaiveExponentiation}($k+1$) is at most $1+(k-1)=(k+1)-1$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc NaiveExponentiation} algorithm uses at most $n-1$ multiplications for all $n\in\mathbb{N}$.\\

\textbf{Proof. }

By Lemma I, {\sc NaiveExponentiation}($n$) computes $2024^n$ for all $n\in\mathbb{N}$.

By Lemma II, {\sc NaiveExponentiation}($n$) uses at most $n-1$ multiplications for all $n\in\mathbb{N}$.

Ergo, for all $n\in\mathbb{N}$, we have demonstrated that {\sc NaiveExponentiation}($n$) computes $2024^n$ using at most $n-1$ multiplications.$~\square$
\end{solution}
\newpage
    \item Using $O(\log_2 n)$ many multiplications, assuming $n$ is a power of $2$, i.e., $n=2^k$.
\begin{solution}
{\sc SquareExponentiation}($n$) where $n=2^x$ for $x\geq 0$:
\begin{itemize}
\item if $n=2^0$, then return $2024$;
\item otherwise, compute $y~=~${\sc SquareExponentiation}($n/2$), and return $y\times y$.
\end{itemize}
\textbf{Lemma I. }

\textit{Claim. }{\sc SquareExponentiation}($n$)$~=2024^n$ for $n=2^x$ where $x\geq 0$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=2^0$. We have {\sc SquareExponentiation}($2^0$)$~=2024=2024^1$. The claim holds in the base case.

\textit{Hypothesis. }Consider $n=2^k$ where $k>0$. Assume that {\sc SquareExponentiation}($2^k$)$~=2024^{2^k}$.

\textit{Inductive step. }Consider $n=2^{k+1}$. Then

$y=~${\sc SquareExponentiation}($2^{k+1}/2$)$~=~${\sc SquareExponentiation}($2^k$). By the inductive hypothesis, we have
{\sc SquareExponentiation}($2^k$)$~=~2024^{2^k}$. Thus $y=2024^{2^k}$. Now

{\sc SquareExponentiation}($2^{k+1}$)$~=~y\times y$, so {\sc SquareExponentiation}($2^{k+1}$)$~=2024^{2^k}\times 2024^{2^k}=2024^{2^k+2^k}=2024^{2\cdot 2^k}=2024^{2^{k+1}}$, thus completing the inductive step.

Hence, by the principle of mathematical induction, the {\sc SquareExponentiation} algorithm produces the correct result for all $n=2^x$ where $x\geq 0$.\\

\textbf{Lemma II. }

\textit{Claim. }{\sc SquareExponentiation}($n$) uses $O(\log_2n)$ multiplications for $n=2^x$ where $x\geq 0$. Let $M(n)$ denote the number of multiplications used by {\sc Square Exponentiation}($n$). We claim that $M(n)=O(\log_2n)$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=2^0$. Then there are no multiplications, so $M(2^0)=0$. Since $\log_2n\geq 0$ for $n\geq 1$, we can choose any positive constant $C$ and any $n_0\geq 1$ such that $0\leq C\log_2n$ for $n>n_0$. Therefore $M(n)=O(\log_2n)$ holds in the base case.

\textit{Hypothesis. }Consider $n=2^k$ where $k>0$. Assume that $M(2^k)=O(\log_2n)$.

\textit{Inductive step. }Consider $n=2^{k+1}$. Then the number of multiplications is $1$ (to compute $y\times y$), plus the number of multiplications used by the recursive call. That is, 
\begin{align*}
M(2^{k+1})&=1+M\left(\frac{2^{k+1}}{2}\right)\\
&=1+M(2^k).
\end{align*}

By the inductive hypothesis, we have $M(2^k)=O(\log_2n)$. Asymptotically,
\begin{align*}
M(2^{k+1})&=1+O(\log_2n)\\
&=O(1)+O(\log_2n)\\
&=O(\log_2n).
\end{align*}

This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc SquareExponentiation} algorithm uses $O(\log_2n)$ many multiplications for all $n=2^x$ where $x\geq 0$.\\

\textbf{Proof. }

By Lemma I, {\sc SquareExponentiation}($n$) computes $2024^n$ for all $n=2^x$ where $x\geq 0$.

By Lemma II, {\sc SquareExponentiation}($n$) uses at $O(\log_2n)$ many multiplications for all $n=2^x$ where $x\geq 0$.

Ergo, for all $n=2^x$ where $x\geq 0$, we have demonstrated that {\sc SquareExponentiation}($n$) computes $2024^n$ using $O(\log_2n)$ many multiplications.$~\square$
\end{solution}
\newpage
\item Using $O(\log_2 n)$ many multiplications for \emph{any} $n$ (not necessarily a power of $2$).
\begin{solution}\\

\textbf{Algorithm I. }{\sc FastExponentiation}($b,n$) where $n\in\mathbb{N}$:
\begin{itemize}
\item if $n=1$, then return $b$;
\item otherwise, compute $b'=b\times b$, and:
\begin{itemize}
    \item if $n$ is even, then return {\sc FastExponentiation}($b',n/2$);
    \item otherwise, return the product $b~\times~${\sc FastExponentiation}($b',(n-1)/2$).
\end{itemize}
\end{itemize}

\textbf{Algorithm II. }{\sc FastPow2024($n$)} where $n\in\mathbb{N}$: return the result {\sc FastExponentiation}($2024,n$).\\

\textbf{Lemma I.}

\textit{Claim. }{\sc FastExponentiation}($b,n$)$~=b^n$ for all $b,n\in\mathbb{N}$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. We have {\sc FastExponentiation}($b,1$)$~=b=b^1$ for all $b\in\mathbb{N}$. The claim holds in the base case.

\textit{Hypothesis. }Consider $1<n<k$. Assume that {\sc FastExponentiation}($b,k$)$~=b^k$ for all $b\in\mathbb{N}$.

\textit{Inductive step. }Consider $n=k+1$. Since $k+1>1$, we compute $b'=b\times b$. Of course, since $b\in\mathbb{N}$, we have $b'\in\mathbb{N}$. Since $k\in\mathbb{N}$, either $k+1$ is even or $k+1$ is odd:
\begin{itemize}
\item Suppose $k+1$ is even. Now,

{\sc FastExponentiation}($b,k+1$)$~=~${\sc FastExponentiation}($b',\frac{k+1}{2}$).

Since $k+1\in\mathbb{N}$ and $k+1$ is even, we have $\frac{k+1}{2}\in\mathbb{N}$. By the strong induction hypothesis, {\sc FastExponentiation}($b',\frac{k+1}{2}$)$~={b'}^{\frac{k+1}{2}}=(b\times b)^{\frac{k+1}{2}}=(b^2)^{\frac{k+1}{2}}=b^{k+1}$. Ergo

{\sc FastExponentiation}($b,k+1$)$~=b^{k+1}$.
\item Suppose instead $k+1$ is odd. Now,

{\sc FastExponentiation}($b,k+1$)$~=b~\times~${\sc FastExponentiation}($b',\frac{(k+1)-1}{2}$).

Since $k+1\in\mathbb{N}$ and $k+1$ is odd, we have $\frac{(k+1)-1}{2}\in\mathbb{N}$. By the strong induction hypothesis, {\sc FastExponentiation}($b',\frac{(k+1)-1}{2}$)$~={b'}^{\frac{(k+1)-1}{2}}={b'}^{\frac{k}{2}}=(b\times b)^{\frac{k}{2}}=(b^2)^{\frac{k}{2}}=b^k$. Ergo

{\sc FastExponentiation}($b,k+1$)$~=b\times b^k=b^{k+1}$.
\end{itemize}

In all cases, {\sc FastExponentiation}($b,k+1$)$~=b^{k+1}$ for all $b\in\mathbb{N}$. This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc FastExponentiation} algorithm produces the correct result for all $b,n\in\mathbb{N}$.\\

\textbf{Lemma II.}

\textit{Claim. }{\sc FastExponentiation}($b,n$)$~=b^n$ uses $O(\log_2n)$ multiplications for all $b,n\in\mathbb{N}$. Let $M(b,n)$ denote the number of multiplications used by {\sc FastExponentiation}($b,n$). We claim that $M(b,n)=O(\log_2n)$. We can demonstrate the claim by induction on $n$.

\textit{Basis. }Consider $n=1$. Then, regardless of the base $b$, there are no multiplications, so $M(b,1)=0$ for all $b\in\mathbb{N}$. Since $\log_2n\geq 0$ for $n\geq 1$, we can choose any positive constant $C$ and any $n_0\geq 1$ such that $0\leq C\log_2n$ for $n\geq n_0$. Therefore $M(b,n)=O(\log_2n)$ holds for all $b\in\mathbb{N}$ in the base case.

\textit{Hypothesis. }Consider $1<n<k$. Assume that $M(b,k)=O(\log_2 n)$ for all $b\in\mathbb{N}$.

\textit{Inductive step. }Consider $n=k+1$. Since $k+1>1$, we know $1$ multiplication is required to compute $b'=b\times b$. Since $k\in\mathbb{N}$, either $k+1$ is even or $k+1$ is odd:
\begin{itemize}
\item Suppose $k+1$ is even. The number of additional multiplications is $M\left(b',\frac{k+1}{2}\right)$. By the strong induction hypothesis, we have $M\left(b',\frac{k+1}{2}\right)=O(\log_2n)$.
\item Suppose instead $k+1$ is odd. The number of additional multiplications is $M\left(b',\frac{(k+1)-1}{2}\right)$. By the strong induction hypothesis, we have $M\left(b',\frac{(k+1)-1}{2}\right)=M\left(b',\frac{k}{2}\right)=O(\log_2n)$.
\end{itemize}
In all cases, the number of additional multiplications is $O(\log_2n)$. Considering also the single multiplication used to compute $b'=b\times b$, asymptotically we have
\begin{align*}
M(b,k+1)&=1+O(\log_2n)\\
&=O(1)+O(\log_2n)\\
&=O(\log_2n).
\end{align*}
This completes the inductive step.

Hence, by the principle of mathematical induction, the {\sc FastExponentiation} algorithm uses $O(\log_2n)$ many multiplications for all $b,n\in\mathbb{N}$.\\

\textbf{Proof.}

By construction, {\sc FastPow2024}($n$) algorithm returns {\sc FastExponentiation}($2024,n$) and uses exactly as many multiplications as the {\sc FastExponentiation}($2024,n$) algorithm.

By Lemma I, {\sc FastExponentiation}($2024,n$) computes $2024^n$ for all $n\in\mathbb{N}$. Therefore, the {\sc FastPow2024} algorithm computes $2024^n$ for all $n\in\mathbb{N}$.

By Lemma II, {\sc FastExponentiation}($2024,n$) uses $O(\log_2n)$ many multiplications for all $n\in\mathbb{N}$. Therefore, the {\sc FastPow2024} algorithm uses $O(\log_2n)$ many multiplications for all $n\in\mathbb{N}$.

Ergo, for all $n\in\mathbb{N}$, we have demonstrated that {\sc FastPow2024}($n$) computes $2024^n$ using $O(\log_2n)$ many multiplications.$~\square$
\end{solution}
\newpage
\end{enumerate}
\subsection*{Question 3: Close to sorted}
Suppose you are given a list $A$ of $n$ distinct numbers. You are guaranteed that this list is `close to sorted' in the following sense: if $A_{\text{sorted}}$ denotes the list fully sorted in increasing order, then $A_{\text{sorted}}$ differs from $A$ in at most $\log_2 n$ many positions. Intuitively, this says that at most $\log_2 n$ elements are out of place in the original list. For instance, in the list 
\[
(2,5,9,11,20,14,15,12,25,30)
\]
only two elements are out of place (20 and 12), since after sorting, we get 
\[
(2,5,9,11,12,14,15,20,25,30) \;.
\]
The following exercises will ultimately lead to an algorithm which sorts $A$ in time $O(n)$~\footnote{Note that sorting algorithms take $O(n\log n)$ time without any assumptions on the inputs. Here, we are able to get the faster $O(n)$ run-time by \emph{assuming} that the input array $A$ is already close to being sorted.}. Answer each exercise.

\begin{enumerate}
    \item Prove that the leftmost (first) out of place element must be too big (not too small) for its place.
\begin{solution}
\textit{Claim. }Let $k$ represent the position of the leftmost out-of-place element in $A$. Then $A[k]$ is too big (not too small) for its position.

\textit{Proof. }Assume, for the sake of contradiction, that $k$ is too small for its position. Since $A[k]$ is too small for its position, we know $k>1$. Otherwise, if $k=1$, it could not be too small for its position. Of course, since $A[k]$ is too small for its position, $A[k]<A[k-1]$.

This implies that $A[k-1]>A[k]$, meaning that $A[k-1]$ is too big for its place. $A[k-1]$ is out of place and $k-1<k$. Therefore, $A[k-1]$ is out of place and \textit{further left} than $A[k]$. This contradicts the hypothesis that $A[k]$ is the leftmost out-of-place element in $A$.

We conclude that the leftmost out-of-place element in $A$ must \textit{not} be too small for its place. Hence, the leftmost out-of-place element in $A$ must be too big for its place.$~\square$
\end{solution}
    \item Suppose we construct a stack $S$ as follows: We go through $A$ from left to right, pushing elements one by one into $S$ as long as the next element from $A$ is larger than the top element $s$ of the stack so far. If at any step $i$ the element $A[i]$ which we are currently considering in $A$ is smaller than $s$, we instead pop $s$ from $S$ and continue.
    This idea is described in the pseudo code below:
    \begin{minipage}{\linewidth}
    \begin{algorithm}[H]
    \label{incsubseq}
    \caption{Construct increasing subsequence $S$}
    \begin{algorithmic}
    \REQUIRE $A$ and an empty stack $S$.
    \ENSURE $S$ representing an increasing subsequence of $A$
    \STATE $n \leftarrow \text{len}(A)$
    \FOR {$i = 1 \text{ to } n$}
        \IF {$S$ is empty \OR $S.\text{topElement}() \leq A[i]$}
        \STATE $S.\text{push}(A[i])$
        \ELSE 
        \STATE $S.\text{pop}()$
        \ENDIF
    \ENDFOR
    \STATE \RETURN $S$
    \end{algorithmic}
    \end{algorithm}
    \end{minipage}
    \smallskip
    
    If $A$ is initially $(2,5,9,11,20,14,15,12,25,30)$, what will $S$ be after Algorithm 1 has run?
\begin{solution}
Let $A=(2,5,9,11,20,14,15,12,25,30)$. Let $S$ be an empty stack and $S_i$ represent the stack at the end of iteration $1\leq i\leq\text{len}(A)$. Stepping through the algorithm:
\begin{align*}
S_1&=(2)&\textit{since $S$ is empty before iteration $i=1$.}\\
S_2&=(2,5)&\textit{since $2\leq 5$.}\\
~&\vdots&\vdots\\
S_5&=(2,5,9,11,20)&\textit{since $5\leq 9\leq 11\leq 20$.}\\
S_6&=(2,5,9,11)&\textit{since $S$ is not empty, $20\nleq 14$.}\\
S_7&=(2,5,9,11,15)&\textit{since $11\leq 15$.}\\
S_8&=(2,5,9,11)&\textit{since $S$ is not empty, $15\nleq 12$.}\\
~&\vdots&\vdots\\
S_{10}&=(2,5,9,11,25,30)&\textit{since $11\leq 25\leq 30$.}
\end{align*}
After Algorithm 1 has run, $S=(2,5,9,11,25,30)$.
\end{solution}
    \item Prove that $S$ is an increasing subsequence of $A$ (the elements of the output stack are increasing from bottom to top).
\begin{solution}

\end{solution}
    
    
    \item Prove that $S$ contains all elements of $A$ except at most $2\log_2 n$. (Hint: show that every time we leave out a pair of elements (the ``else'' case) at least one of them must have been an out of place element.)

\begin{solution}   INSERT YOUR SOLUTION HERE   \end{solution}

    
    \item Use the previous statements to design and prove the correctness and run-time of an algorithm to sort the nearly sorted array in time $O(n)$. 
    
    \hint{Suppose you could split $A$ into a large sorted subsequence and only a few remaining unsorted elements in time $O(n)$. (This is what parts 3 and 4 show.) Now think of the ``Merge'' subroutine in Merge Sort.}

\begin{solution}   INSERT YOUR SOLUTION HERE   \end{solution}
\end{enumerate}





    



\subsection*{Honors Question: The Counterfeit Gem}
Suppose you strike up a relationship with a jeweler and get a bulk deal on $n$ diamonds. However, you were warned beforehand that one (and only one) of the diamonds is a convincing fake, and the only difference between it and the real gems is that the fake one's weight is different, while all real ones have the same weight. Luckily, you have a balance scale at home, which can test two (disjoint) subsets of the diamonds and tell us which is lighter. 

\begin{enumerate}
    \item $(**)$ Assume that you know that the fake one is lighter. Give an algorithm that finds the counterfeit gem in $O(\log n)$ uses of the scale. Justify the correctness and time complexity of your proposed algorithm.

\begin{solution}   INSERT YOUR SOLUTION HERE   \end{solution}

    \item $(****)$ Now consider the general case where you only know it has a different weight from the real ones. What can you do in this case? Can you still do it with $O(\log n)$ measurements? For concrete numbers, consider $n=12$ and show how you can identify the fake one only using the scale $3$ times.

\begin{solution}   INSERT YOUR SOLUTION HERE   \end{solution}
\end{enumerate}

